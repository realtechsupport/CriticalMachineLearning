{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPy9XjjyeAupgbD3L6FrVif"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"0KMvlEEKVOzt"},"outputs":[],"source":["# Computational Media II\n","# Assignment #1\n","# What is the chance of survival ?\n","# Naive Bayes classification applied to the Titanic dataset\n","# Due date: Tuesday, Feb 21st, noon.\n","\n","# Implementation approaches gleaned from: https://www.youtube.com/watch?v=PPeaRc-r1OI&ab_channel=codebasics "]},{"cell_type":"markdown","source":["This assignment asks you to reflect on the significance of input parameters for model behavior, exemplified on Naive Bayes Classification (via the scikit library) on the Titanic data set.\n","\n","\n","Your tasks are as follows:\n","\n","1) Create at least two different Naive Bayes models.\n","\n","Each model must have a different configuration of inputs (at least one column must be different). Describe your rational for the choice of inputs. Describe how you wrangle your data to ensure clean inputs to the Naive Bayes classifier, and implement it in code.\n","\n","2) Evaluate the behavior of your models on imagined passengers (as the example below shows). Compare the performance of your two classifiers and describe the results.\n","\n","3) Search the Kaggle repository (https://www.kaggle.com/) for additional datasets that you could imagine applying Naive Bayes classification on.\n","Describe which aspects of the selected dataset are of interest to you. Describe which features would be inputs and which would be the target."],"metadata":{"id":"wCVvVfbFbNwS"}},{"cell_type":"code","source":["import os, sys\n","import numpy\n","import pandas\n","\n","# The Titanic dataset\n","source = 'https://raw.githubusercontent.com/realtechsupport/CriticalMachineLearning/main/various_datasets/titanic_train.csv'\n","# Load data as a dataframe\n","dataframe = pandas.read_csv(source, sep = ',')\n","# Show first 5 rows\n","dataframe.head(5)"],"metadata":{"id":"gryAXqKfVmF6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Show dimensions\n","dataframe.shape"],"metadata":{"id":"I69f-f8uYQoU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# CHANGE THIS SELECTION !!!\n","\n","dataframe.drop(['PassengerId', 'Name', 'SibSp', 'Parch', 'Cabin', 'Ticket', 'Embarked'], axis='columns', inplace=True)\n","dataframe.head()"],"metadata":{"id":"Z4-3c30O5YyI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# NO CHANGE HERE\n","\n","# Define target and dependent variables in two separate entities\n","target = dataframe.Survived\n","inputs = dataframe.drop('Survived', axis = 'columns')"],"metadata":{"id":"VKpnUYz8fQtj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ADAPT this feature to suit your data choices\n","\n","# We will use a version of one-hot encoding to represent gender\n","# N possible values for a categorical variable, will necessitate N columns for representation\n","# Male and female are mapped to two columns\n","# Convert gender to a boolean using the pandas functionality \"get_dummies\"\n","dummies = pandas.get_dummies(inputs.Sex)\n","dummies.head(3)"],"metadata":{"id":"NiJsAws1fyi7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ADJUST this change to suit your inputs\n","\n","# Append the boolean representation of gender to the existing inputs\n","# Also drop the gender column as it is no longer needed\n","inputs = pandas.concat([inputs, dummies], axis='columns')\n","inputs.drop('Sex', axis = 'columns', inplace=True)\n","inputs.head(3)"],"metadata":{"id":"7BfIYDFkgMm3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check the dataset for any missing data\n","inputs.columns[inputs.isna().any()]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FUVBwQZVjk3w","executionInfo":{"status":"ok","timestamp":1676416206219,"user_tz":300,"elapsed":256,"user":{"displayName":"m b","userId":"01661067310575909085"}},"outputId":"4d13c8c7-fb42-4f4f-d0cc-65b5bac7c0f9"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Index(['Age'], dtype='object')"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["# Now find those AGE NaN values\n","inputs[inputs.isna().any(axis=1)]"],"metadata":{"id":"Ss54G7EHmFsm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# EXPERIMENT with different ways of addressing missing values\n","# average / removal / data from different source, etc\n","# Defend your approach\n","\n","# That is alot of missing values.\n","# Here is a trick - fill in the values with imaginary numbers\n","# Take the average age as a guess\n","inputs.Age = inputs.Age.fillna(inputs.Age.mean())\n","inputs.head(10)\n","\n","#Can you imagine a better way to do this?"],"metadata":{"id":"65X43QQnmcNC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# CHANGE test percentage to see how training and test size choices impact the classifier\n","\n","# OK, now we have a mathematically complete dataset, albeit one that makes some really strong assumptions...\n","# We can create training and testing sets on the data now.\n","# Check out the sklearn library\n","# https://scikit-learn.org/stable/\n","\n","from sklearn.model_selection import train_test_split\n","test_percentage = 0.2\n","X_train, X_test, y_train, y_test = train_test_split(inputs, target, test_size = test_percentage)"],"metadata":{"id":"df2XYHVunAAO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check the training and test sets\n","print(len(X_train))\n","print(len(X_test))\n","X_train[0:5]"],"metadata":{"id":"tDlnPtz5nrGG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_test[0:5]"],"metadata":{"id":"dxrS5Ez4o33e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# NO CHANGE required here\n","\n","# Create the Naive Bayes Model, and use a Gaussian distribution \n","# https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html\n","\n","from sklearn.naive_bayes import GaussianNB\n","model = GaussianNB()"],"metadata":{"id":"dgyrW8H2oSLG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# NO CHANGE required here\n","\n","# Train the model on the training data\n","model.fit(X_train, y_train)"],"metadata":{"id":"qyNuNue4pkgi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# NO CHANGE required here\n","\n","# How good is the performance on the test set?\n","model.score(X_test, y_test)"],"metadata":{"id":"TAEPrmObp2LQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# NO CHANGE required here\n","\n","# That meas about 77% accuracy with a 80 / 20 train test data split\n","# Now you can use the trained model to predict how one of the test cases would be evaluated\n","# In other words, if they would survive, based on the model\n","\n","# Lets try the first 5 cases\n","model.predict(X_test[:5])\n"],"metadata":{"id":"__5mwnFC0rcR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# NO CHANGE required here\n","\n","# 1 means survive, 0 means die\n","# Compare to the ground truth data, y_test\n","y_test[:5]"],"metadata":{"id":"du1ZIfk31PQL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Ah ha, we have one false prediction (#100)\n","# If we retrain the network with more data, and use less for testing,\n","# We might get better results..."],"metadata":{"id":"ffmL9y871neo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# NO CHANGE required here\n","\n","# We can use label encoder to translate the 0/1 back to text\n","from sklearn import preprocessing\n","le = preprocessing.LabelEncoder()\n","le.fit([\"survived\", \"died\"])\n","list(le.inverse_transform(y_test[:5]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sCjyAC6t4cQT","executionInfo":{"status":"ok","timestamp":1676416787053,"user_tz":300,"elapsed":261,"user":{"displayName":"m b","userId":"01661067310575909085"}},"outputId":"10978dc8-4376-48ec-c07f-5c96b268e95d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['survived', 'died', 'died', 'survived', 'died']"]},"metadata":{},"execution_count":27}]},{"cell_type":"code","source":["# NO CHANGE required here\n","\n","# If you want the probabilities of survival (instead of a live/die result),\n","# use the predict_proba() function\n","model.predict_proba(X_test[:5])"],"metadata":{"id":"ZDM9SNRL2Q3y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# The first number is the probability of dying, the second the probability of survival, based on the current model"],"metadata":{"id":"OtYoZk272qgn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# HERE is where you compare model_1 with model_2\n","\n","# Write a function that takes in the features of an imagined passenger, and a model\n","# and returns the prediction\n","# Include a loop so you can process multiple passengers\n","\n","# features of an imagined passenger\n","pclass = 2\n","age = 15\n","fare = 1000\n","female = 0\n","male = 1\n","imagined = [pclass, age, fare, female, male]\n","a_imagined = numpy.array(imagined)\n","\n","prediction_imagined_passenger = model.predict([a_imagined])\n","list(le.inverse_transform(prediction_imagined_passenger))"],"metadata":{"id":"LWnvmpbT83OF"},"execution_count":null,"outputs":[]}]}