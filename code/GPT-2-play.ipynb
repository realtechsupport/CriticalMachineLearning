{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"GPT-2-play.ipynb","provenance":[{"file_id":"1DUaP3Mc4-op0Fj04Qoe6s1eeJrPBx8TV","timestamp":1649856180514},{"file_id":"https://github.com/ilopezfr/gpt-2/blob/master/gpt-2-playground_.ipynb","timestamp":1643386903851}],"private_outputs":true,"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["# Experiments with GPT-2 on Colab\n","# Jan / April 2022"],"metadata":{"id":"dH9-36whLcLC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# original source: https://colab.research.google.com/github/ilopezfr/gpt-2/blob/master/gpt-2-playground_.ipynb\n","# also check: https://colab.research.google.com/github/sarthakmalik/GPT2.Training.Google.Colaboratory/blob/master/Train_a_GPT_2_Text_Generating_Model_w_GPU.ipynb\n","# https://towardsdatascience.com/step-by-step-guide-on-how-to-train-gpt-2-on-books-using-google-colab-b3c6fa15fef0\n","# https://colab.research.google.com/drive/1Zbq6vcctq4yysOTj-QN8-XC2FZ5gRTSR?usp=sharing\n","# https://towardsdatascience.com/conditional-text-generation-by-fine-tuning-gpt-2-11c1a9fc639d\n","\n","#  >> https://jaketae.github.io/study/gpt2/"],"metadata":{"id":"ODm0Czrymu6s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# I made the following changes to the /content/gpt-2/reqs.txt file:\n","#fire>=0.1.3\n","#regex==2018.1.10\n","#requests==2.23.0\n","#tqdm>=4.48.0\n","#folium==0.2.1\n","\n","# and changed model path in generate_unconditional_samples.py\n","#previously\n","#models_dir = os.path.expanduser(os.path.expandvars(models_dir))\n","#now\n","#models_dir = '/content/gpt-2/models'"],"metadata":{"id":"7tW-MQanDwfP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Before starting, set Runtime Type to GPU on the top menu bar.\n","# Installation: Clone the repo, install dependencies, and download the model weights.\n","# You can choose between the small 117M, medium 345M, large 774M model, xl 1.5B model or all of them."],"metadata":{"id":"VcaTS-q4OeE8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# steps 1 - 3 should be done only once (unless you delete the gpt-2 folder to save memory space)"],"metadata":{"id":"Xt0OLa_cMLST"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Here some results from my own tests:\n","\n","INPUT TEXT\n","\n","Alan: \"I need some cash right now\".\n","\n","Sally: \"I don't have any money. I don't have money for you. Last time I gave you some money, you lost it in a night of wild gambling.\"\n","\n","Alan: \"This time is different.\"\n","\n","Sally: \"How is this time different. Tell me.\"\n","\n","\n","+++++++++++++++++++++++++++++++++++++++++++++++\n","\n","\n","-> output Runway ML (max chars 300, TopP 0.9, seed 917\n","\n","Alan: \"Fine. Give it to me. Give it to me. Give it to me. The hard part is out. You've decided. Now what?\"\n","\n","Sally: \"And you've decided that it's all going to turn into a fucking disaster. And I'm the one who's going to find out when you need me\".\n","(And it all ends on SNL) 11/29/94\n","\n","+++++++++++++++++++++++++++++++++++++++++++++++\n","\n","Using GPT small model on CoLab with same input:\n","\n","-> output\n","\n","Alan: \"Don't tell me.\" \n","\n","Sally: \"Don't listen to the people. Have fun! They've got what they want.\" \n","\n","Alan: \"I'm not giving money, you and your kids.\" \n","\n","Sally: \"You're not gonna tell me anything. I want you to live in a nice house for two hours a day. They can fix your house. This is for your protection. This is for your safety!\"\n","\n","Alan: \"I'm not giving your money back then. That would ruin everyone.\" \n","\n","Sally: \"But then you can't kill us. We'll be with them."],"metadata":{"id":"sf46cSuvF8Yy"}},{"cell_type":"code","source":["#1 make sure there is no old gpt-2 directory ... if necessary, delete old install\n","try:\n","  !rm -rf /content/gpt-2\n","except:\n","  pass"],"metadata":{"id":"VqSKxo_D65DW"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FKqlSCrpS9dH"},"source":["#2\n","!git clone https://github.com/ilopezfr/gpt-2/"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#3\n","import os\n","os.chdir('/content/gpt-2')\n","#Download model weights\n","!python download_model.py 117M\n","\n","# !python download_model.py 345M\n","# !python download_model.py 774M\n","# !python download_model.py 1558M"],"metadata":{"id":"vwZ74U-o8yyH"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c8f1U3QqOrUk"},"source":["#force to tensorflow 1\n","%tensorflow_version 1.x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Za4oaCp3Otzg"},"source":["#check the version\n","import tensorflow\n","print(tensorflow.__version__)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Be3La1bOGLnr"},"source":["import os\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'   # disable all debugging logs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"brNPcGyQGP0L"},"source":["#> edit reqs.txt tqdm>=4.48.0\n","# downloand, edit and upload to same directory..\n","\n","# then issue the following command...\n","!pip3 -q install -r /content/gpt-2/reqs.txt"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import warnings\n","warnings.filterwarnings(\"ignore\")"],"metadata":{"id":"TUt9JriJilUw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LVlv3gyGTkYd"},"source":["###  2. Unconditional sample generation\n","\n","*WARNING: Samples are unfiltered and may contain offensive content.*\n","\n","To generate unconditional samples from the small model:\n","```\n","!python3 src/generate_unconditional_samples.py\n","```\n","There are a few flags available, with a default value: \n","-  `**model_name = '1558M' ` : choose between 117M, 345M, 774M, and 1558M models. If not specified, the default is 117M. **\n","- `seed = None`  || a random value is generated unless specified. give a specific integer value if you want to reproduce same results in the future.\n","- `nsamples = 1`     ||  specify the number of samples you want to print\n","- `length = None`   ||  number of tokens (words) to print on each sample.\n","- `batch_size= 1`  ||  how many inputs you want to process simultaneously. *only affects speed/memory* \n","- `temperature = 1`  ||  float between 0 and 1. scales logits before sampling prior to softmax. higher temperature results in more random completions.\n","- `top_k = 0`   ||  Integer value controlling diversity.  Truncates the set of logits considered to those with the highest values. 1 means only 1 word is considered for each step (token), resulting in deterministic completions. 40 means 40 words are considered at each step. 0 (default) is a special setting meaning no restrictions. 40 generally is a good value.\n","\n","*Note: This part takes a while (~5min) until it starts printing gpt2 samples*\n"]},{"cell_type":"code","source":["#!python3  /content/gpt-2/src/generate_unconditional_samples.py --model_name='117M' --nsamples=1 --length=200 --top_k=40 --temperature=0.7 \n","!python3  /content/gpt-2/src/generate_unconditional_samples.py --model_name='117M' --nsamples=2 --top_k=40\n","#!python3  /content/gpt-2/src/generate_unconditional_samples.py --model_name='117M' --nsamples=2 --top_k=80  "],"metadata":{"id":"MuY4I2185baE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JAyrz11CWmZI"},"source":["## Conditional sample generation\n","\n","To generate conditional samples from the small model:\n","```\n","!python3 src/interactive_conditional_samples.py\n","```\n","It comes with a few flags available, with a default value: \n","-  `model_name = '117M' ` : choose between 117M and 345M models. By default is 117M. \n","- `seed = None`  || a random value is generated unless specified. give a specific integer value if you want to reproduce same results in the future.\n","- `nsamples = 1`     ||  specify the number of samples you want to print\n","- `length = None`   ||  number of tokens (words) to print on each sample.\n","- `batch_size= 1`  ||  how many inputs you want to process simultaneously. *only affects speed/memory* \n","- `temperature = 1`  ||  float between 0 and 1. scales logits before sampling prior to softmax. higher temperature results in more random completions.\n","- `top_k = 0`   ||  Integer value controlling diversity.  Truncates the set of logits considered to those with the highest values. 1 means only 1 word is considered for each step (token), resulting in deterministic completions. 40 means 40 words are considered at each step. 0 (default) is a special setting meaning no restrictions. 40 generally is a good value.\n","\n","++ You can use this module to make GPT-2 generate a text response to an input text.  Type or copy paste your input text into the flashing box (double click to activate) ++\n"]},{"cell_type":"code","source":["# edit interactive_conditional_samples.py: models_dir = '/content/gpt-2/models'\n","!python3 /content/gpt-2/src/interactive_conditional_samples.py  --model_name='117M' --top_k=40 --nsamples=1 --length=128"],"metadata":{"id":"EiKRGIEs5LJO"},"execution_count":null,"outputs":[]}]}