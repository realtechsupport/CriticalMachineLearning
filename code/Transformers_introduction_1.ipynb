{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPwJdbBYFpJpX6zplG/lDfU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"m6BU1KWlu9Vh"},"outputs":[],"source":["# Transformers\n","\n","#Videos\n","#short: https://www.youtube.com/watch?v=ZXiruGOCn9s&ab_channel=IBMTechnology\n","#intermediate: https://www.youtube.com/watch?v=4Bdc55j80l8&ab_channel=TheAIHacker\n","#long: https://www.youtube.com/watch?v=bCz4OMemCcA&ab_channel=UmarJamil\n","\n","#Descriptions\n","#original paper:  Vawani et al. Attention is all you need. 2017.\n","#https://arxiv.org/pdf/1706.03762.pdf\n","\n","#annotated:\n","# https://nlp.seas.harvard.edu/annotated-transformer/\n","\n","#Tutorials and background reading\n","## attention mechanism\n","# https://towardsdatascience.com/attention-and-transformer-models-fe667f958378\n","\n","\n","#positional encoding concept:\n","#https://kazemnejad.com/blog/transformer_architecture_positional_encoding/\n","\n","\n","# https://jalammar.github.io/illustrated-transformer/\n","\n","#https://quantdare.com/transformers-is-attention-all-we-need-in-finance-part-i/\n","\n","#https://pub.aimind.so/transformer-model-and-variants-of-transformer-chatgpt-3d423676e29c\n","\n","\n","\n","\n","#Notebooks ---------------------------------------------------------------------\n","#transformers_introduction_1.ipynb\n","#https://colab.research.google.com/drive/1CsRjUNFpe0trdZ74OeRg37TmLYq6NSYb\n","\n","#transformers_introduction_2.ipynb\n","#https://colab.research.google.com/drive/1kUsoRpyGAKrrb853vvKytl1yJo_Ji9DI\n","\n","#Transformers-Huggingface.ipynb\n","#https://colab.research.google.com/drive/1NcRL2lY91dL2MN6ndfd6UMN9KM0JYigv\n","\n","#Huggingface_Transformers_Casestudy.ipynb\n","#https://colab.research.google.com/drive/1k-WH26sKjjOGPhWSfriBfY1neZwRdaO3\n","\n","\n","#-------------------------------------------------------------------------------\n","# base model in pytorch\n","# https://towardsdatascience.com/a-detailed-guide-to-pytorchs-nn-transformer-module-c80afbc9ffb1\n","# https://nlp.seas.harvard.edu/2018/04/03/attention.html\n","\n","# Nvidia version of the transformer model\n","# https://blogs.nvidia.com/blog/2022/03/25/what-is-a-transformer-model/\n","\n","# (first two images from:  https://colah.github.io/posts/2015-08-Understanding-LSTMs/\n","# third image from Vawani paper\n","\n","# Code scratched from:\n","# Daniel Melchor: https://medium.com/@danielmelchor/a-detailed-guide-to-pytorchs-nn-transformer-module-c80afbc9ffb1\n","\n","# Vision transformers\n","# https://github.com/google-research/vision_transformer\n","\n"]},{"cell_type":"markdown","source":["<img src=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png\" width=\"600\">\n","\n","<br><br>\n","\n","<img src=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png\" width=\"600\">\n","\n","<br><br>\n","\n","<img src=\"https://quantdare.com/wp-content/uploads/2021/11/transformer_arch.png\" width=\"600\">"],"metadata":{"id":"BbeugTiQ3mA0"}},{"cell_type":"code","source":["\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","import math\n","import numpy as np"],"metadata":{"id":"j3phdXJ_v6si"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Transformer(nn.Module):\n","\n","    # Constructor\n","    def __init__(\n","        self,\n","        num_tokens,\n","        dim_model,\n","        num_heads,\n","        num_encoder_layers,\n","        num_decoder_layers,\n","        dropout_p,\n","    ):\n","        super().__init__()\n","\n","        # INFO\n","        self.model_type = \"Transformer\"\n","        self.dim_model = dim_model\n","\n","        # LAYERS\n","        self.positional_encoder = PositionalEncoding(\n","            dim_model=dim_model, dropout_p=dropout_p, max_len=5000\n","        )\n","        self.embedding = nn.Embedding(num_tokens, dim_model)\n","        self.transformer = nn.Transformer(\n","            d_model=dim_model,\n","            nhead=num_heads,\n","            num_encoder_layers=num_encoder_layers,\n","            num_decoder_layers=num_decoder_layers,\n","            dropout=dropout_p,\n","        )\n","        self.out = nn.Linear(dim_model, num_tokens)\n","\n","    def forward(self, src, tgt, tgt_mask=None, src_pad_mask=None, tgt_pad_mask=None):\n","        # Src size must be (batch_size, src sequence length)\n","        # Tgt size must be (batch_size, tgt sequence length)\n","\n","        # Embedding + positional encoding - Out size = (batch_size, sequence length, dim_model)\n","        src = self.embedding(src) * math.sqrt(self.dim_model)\n","        tgt = self.embedding(tgt) * math.sqrt(self.dim_model)\n","        src = self.positional_encoder(src)\n","        tgt = self.positional_encoder(tgt)\n","\n","        # We could use the parameter batch_first=True, but our KDL version doesn't support it yet, so we permute\n","        # to obtain size (sequence length, batch_size, dim_model),\n","        src = src.permute(1,0,2)\n","        tgt = tgt.permute(1,0,2)\n","\n","        # Transformer blocks - Out size = (sequence length, batch_size, num_tokens)\n","        transformer_out = self.transformer(src, tgt, tgt_mask=tgt_mask, src_key_padding_mask=src_pad_mask, tgt_key_padding_mask=tgt_pad_mask)\n","        out = self.out(transformer_out)\n","\n","        return (out)\n","\n","    def get_tgt_mask(self, size) -> torch.tensor:\n","        # Generates a square matrix where the each row allows one word more to be seen\n","        mask = torch.tril(torch.ones(size, size) == 1) # Lower triangular matrix\n","        mask = mask.float()\n","        mask = mask.masked_fill(mask == 0, float('-inf')) # Convert zeros to -inf\n","        mask = mask.masked_fill(mask == 1, float(0.0)) # Convert ones to 0\n","\n","        # EX for size=5:\n","        # [[0., -inf, -inf, -inf, -inf],\n","        #  [0.,   0., -inf, -inf, -inf],\n","        #  [0.,   0.,   0., -inf, -inf],\n","        #  [0.,   0.,   0.,   0., -inf],\n","        #  [0.,   0.,   0.,   0.,   0.]]\n","\n","        return (mask)\n","\n","    def create_pad_mask(self, matrix: torch.tensor, pad_token: int) -> torch.tensor:\n","        # If matrix = [1,2,3,0,0,0] where pad_token=0, the result mask is\n","        # [False, False, False, True, True, True]\n","        return ((matrix == pad_token))"],"metadata":{"id":"0maRiy-7v7iH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class PositionalEncoding(nn.Module):\n","    def __init__(self, dim_model, dropout_p, max_len):\n","        super().__init__()\n","        # Modified version from: https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n","        # max_len determines how far the position can have an effect on a token (window)\n","\n","        # Info\n","        self.dropout = nn.Dropout(dropout_p)\n","\n","        # Encoding - From formula\n","        pos_encoding = torch.zeros(max_len, dim_model)\n","        positions_list = torch.arange(0, max_len, dtype=torch.float).view(-1, 1) # 0, 1, 2, 3, 4, 5\n","        division_term = torch.exp(torch.arange(0, dim_model, 2).float() * (-math.log(10000.0)) / dim_model) # 1000^(2i/dim_model)\n","\n","        # PE(pos, 2i) = sin(pos/1000^(2i/dim_model))\n","        pos_encoding[:, 0::2] = torch.sin(positions_list * division_term)\n","\n","        # PE(pos, 2i + 1) = cos(pos/1000^(2i/dim_model))\n","        pos_encoding[:, 1::2] = torch.cos(positions_list * division_term)\n","\n","        # Saving buffer (same as parameter without gradients needed)\n","        pos_encoding = pos_encoding.unsqueeze(0).transpose(0, 1)\n","        self.register_buffer(\"pos_encoding\",pos_encoding)\n","\n","    def forward(self, token_embedding: torch.tensor) -> torch.tensor:\n","        # Residual connection + pos encoding\n","        return (self.dropout(token_embedding + self.pos_encoding[:token_embedding.size(0), :]))"],"metadata":{"id":"5nobmCSPwHcZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import random\n","\n","def generate_random_data(n):\n","    SOS_token = np.array([2])\n","    EOS_token = np.array([3])\n","    length = 8\n","\n","    data = []\n","\n","    # 1,1,1,1,1,1 -> 1,1,1,1,1\n","    for i in range(n // 3):\n","        X = np.concatenate((SOS_token, np.ones(length), EOS_token))\n","        y = np.concatenate((SOS_token, np.ones(length), EOS_token))\n","        data.append([X, y])\n","\n","    # 0,0,0,0 -> 0,0,0,0\n","    for i in range(n // 3):\n","        X = np.concatenate((SOS_token, np.zeros(length), EOS_token))\n","        y = np.concatenate((SOS_token, np.zeros(length), EOS_token))\n","        data.append([X, y])\n","\n","    # 1,0,1,0 -> 1,0,1,0,1\n","    for i in range(n // 3):\n","        X = np.zeros(length)\n","        start = random.randint(0, 1)\n","\n","        X[start::2] = 1\n","\n","        y = np.zeros(length)\n","        if X[-1] == 0:\n","            y[::2] = 1\n","        else:\n","            y[1::2] = 1\n","\n","        X = np.concatenate((SOS_token, X, EOS_token))\n","        y = np.concatenate((SOS_token, y, EOS_token))\n","\n","        data.append([X, y])\n","\n","    np.random.shuffle(data)\n","\n","    return (data)\n","\n","\n","def batchify_data(data, batch_size=16, padding=False, padding_token=-1):\n","    batches = []\n","    for idx in range(0, len(data), batch_size):\n","        # We make sure we dont get the last bit if its not batch_size size\n","        if idx + batch_size < len(data):\n","            # Here you would need to get the max length of the batch,\n","            # and normalize the length with the PAD token.\n","            if padding:\n","                max_batch_length = 0\n","\n","                # Get longest sentence in batch\n","                for seq in data[idx : idx + batch_size]:\n","                    if len(seq) > max_batch_length:\n","                        max_batch_length = len(seq)\n","\n","                # Append X padding tokens until it reaches the max length\n","                for seq_idx in range(batch_size):\n","                    remaining_length = max_bath_length - len(data[idx + seq_idx])\n","                    data[idx + seq_idx] += [padding_token] * remaining_length\n","\n","            batches.append(np.array(data[idx : idx + batch_size]).astype(np.int64))\n","\n","    print(f\"{len(batches)} batches of size {batch_size}\")\n","\n","    return (batches)\n","\n","\n","train_data = generate_random_data(9000)\n","val_data = generate_random_data(3000)\n","\n","train_dataloader = batchify_data(train_data)\n","val_dataloader = batchify_data(val_data)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V4q80p6uw8aV","executionInfo":{"status":"ok","timestamp":1712088377063,"user_tz":240,"elapsed":447,"user":{"displayName":"m b","userId":"01661067310575909085"}},"outputId":"ac752576-97be-480d-b485-fec2f638116d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["562 batches of size 16\n","187 batches of size 16\n"]}]},{"cell_type":"code","source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model = Transformer(num_tokens=4, dim_model=8, num_heads=2, num_encoder_layers=3, num_decoder_layers=3, dropout_p=0.1).to(device)\n","opt = torch.optim.SGD(model.parameters(), lr=0.01)\n","loss_fn = nn.CrossEntropyLoss()"],"metadata":{"id":"9PPXsT2lxRRs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1712088411944,"user_tz":240,"elapsed":3191,"user":{"displayName":"m b","userId":"01661067310575909085"}},"outputId":"44108f33-e7ee-43d6-f09c-c5b73f012fad"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n","  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"]}]},{"cell_type":"code","source":["def train_loop(model, opt, loss_fn, dataloader):\n","\n","    model.train()\n","    total_loss = 0\n","\n","    for batch in dataloader:\n","        X, y = batch[:, 0], batch[:, 1]\n","        X, y = torch.tensor(X).to(device), torch.tensor(y).to(device)\n","\n","        # Now we shift the tgt by one so with the <SOS> we predict the token at pos 1\n","        y_input = y[:,:-1]\n","        y_expected = y[:,1:]\n","\n","        # Get mask to mask out the next words\n","        sequence_length = y_input.size(1)\n","        tgt_mask = model.get_tgt_mask(sequence_length).to(device)\n","\n","        # Standard training except we pass in y_input and tgt_mask\n","        pred = model(X, y_input, tgt_mask)\n","\n","        # Permute pred to have batch size first again\n","        pred = pred.permute(1, 2, 0)\n","        loss = loss_fn(pred, y_expected)\n","\n","        opt.zero_grad()\n","        loss.backward()\n","        opt.step()\n","\n","        total_loss += loss.detach().item()\n","\n","    return (total_loss / len(dataloader))\n"],"metadata":{"id":"Qp4HrLooxYlG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def validation_loop(model, loss_fn, dataloader):\n","\n","    model.eval()\n","    total_loss = 0\n","\n","    with torch.no_grad():\n","        for batch in dataloader:\n","            X, y = batch[:, 0], batch[:, 1]\n","            X, y = torch.tensor(X, dtype=torch.long, device=device), torch.tensor(y, dtype=torch.long, device=device)\n","\n","            # Now we shift the tgt by one so with the <SOS> we predict the token at pos 1\n","            y_input = y[:,:-1]\n","            y_expected = y[:,1:]\n","\n","            # Get mask to mask out the next words\n","            sequence_length = y_input.size(1)\n","            tgt_mask = model.get_tgt_mask(sequence_length).to(device)\n","\n","            # Standard training except we pass in y_input and src_mask\n","            pred = model(X, y_input, tgt_mask)\n","\n","            # Permute pred to have batch size first again\n","            pred = pred.permute(1, 2, 0)\n","            loss = loss_fn(pred, y_expected)\n","            total_loss += loss.detach().item()\n","\n","    return (total_loss / len(dataloader))"],"metadata":{"id":"8s0tvDKqxavo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def fit(model, opt, loss_fn, train_dataloader, val_dataloader, epochs):\n","\n","\n","    # Used for plotting later on\n","    train_loss_list, validation_loss_list = [], []\n","\n","    print(\"Training and validating model\")\n","    for epoch in range(epochs):\n","        print(\"-\"*25, f\"Epoch {epoch + 1}\",\"-\"*25)\n","\n","        train_loss = train_loop(model, opt, loss_fn, train_dataloader)\n","        train_loss_list += [train_loss]\n","\n","        validation_loss = validation_loop(model, loss_fn, val_dataloader)\n","        validation_loss_list += [validation_loss]\n","\n","        print(f\"Training loss: {train_loss:.4f}\")\n","        print(f\"Validation loss: {validation_loss:.4f}\")\n","        print()\n","\n","    return (train_loss_list, validation_loss_list)"],"metadata":{"id":"8T6duG1Uxghg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def predict(model, input_sequence, max_length=25, SOS_token=2, EOS_token=3):\n","\n","    model.eval()\n","\n","    y_input = torch.tensor([[SOS_token]], dtype=torch.long, device=device)\n","\n","    num_tokens = len(input_sequence[0])\n","\n","    for _ in range(max_length):\n","        # Get source mask\n","        tgt_mask = model.get_tgt_mask(y_input.size(1)).to(device)\n","\n","        pred = model(input_sequence, y_input, tgt_mask)\n","\n","        next_item = pred.topk(1)[1].view(-1)[-1].item() # num with highest probability\n","        next_item = torch.tensor([[next_item]], device=device)\n","\n","        # Concatenate previous input with predicted best word\n","        y_input = torch.cat((y_input, next_item), dim=1)\n","\n","        # Stop if model predicts end of sentence\n","        if next_item.view(-1).item() == EOS_token:\n","            break\n","\n","    return (y_input.view(-1).tolist())"],"metadata":{"id":"cDZDkh3jyKJ-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_loss_list, validation_loss_list = fit(model, opt, loss_fn, train_dataloader, val_dataloader, 10)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LDLDgydIxlDq","executionInfo":{"status":"ok","timestamp":1712088713040,"user_tz":240,"elapsed":255310,"user":{"displayName":"m b","userId":"01661067310575909085"}},"outputId":"88f211a2-6159-400f-f853-786efdf9ef35"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training and validating model\n","------------------------- Epoch 1 -------------------------\n","Training loss: 0.6243\n","Validation loss: 0.4095\n","\n","------------------------- Epoch 2 -------------------------\n","Training loss: 0.4157\n","Validation loss: 0.3759\n","\n","------------------------- Epoch 3 -------------------------\n","Training loss: 0.3808\n","Validation loss: 0.3316\n","\n","------------------------- Epoch 4 -------------------------\n","Training loss: 0.3516\n","Validation loss: 0.2935\n","\n","------------------------- Epoch 5 -------------------------\n","Training loss: 0.3218\n","Validation loss: 0.2618\n","\n","------------------------- Epoch 6 -------------------------\n","Training loss: 0.2963\n","Validation loss: 0.2319\n","\n","------------------------- Epoch 7 -------------------------\n","Training loss: 0.2697\n","Validation loss: 0.2219\n","\n","------------------------- Epoch 8 -------------------------\n","Training loss: 0.2536\n","Validation loss: 0.1813\n","\n","------------------------- Epoch 9 -------------------------\n","Training loss: 0.2404\n","Validation loss: 0.1746\n","\n","------------------------- Epoch 10 -------------------------\n","Training loss: 0.2331\n","Validation loss: 0.1718\n","\n"]}]},{"cell_type":"code","source":["# Test some examples to observe how the model predicts\n","\n","examples = []\n","examples_tensor = []\n","\n","examples =    [\n","    [2, 0, 1, 3],\n","    [2, 0, 0, 0, 0, 0, 0, 0, 0, 3],\n","    [2, 1, 1, 1, 1, 1, 1, 1, 1, 3],\n","    [2, 1, 0, 1, 0, 1, 0, 1, 0, 3],\n","    [2, 0, 1, 0, 1, 0, 1, 0, 1, 3],\n","    [2, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 3],\n","    [2, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 3]\n","    ]\n","\n","for e in examples:\n","  t = torch.tensor([e], dtype=torch.long, device=device)\n","  examples_tensor.append(t)\n","\n","\n","for idx, example in enumerate(examples_tensor):\n","    result = predict(model, example)\n","    print(f\"Example {idx}\")\n","    print(f\"Input: {example.view(-1).tolist()[1:-1]}\")\n","    print(f\"Continuation: {result[1:-1]}\")\n","    print()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1Oe1OEWmyStb","executionInfo":{"status":"ok","timestamp":1712088758334,"user_tz":240,"elapsed":556,"user":{"displayName":"m b","userId":"01661067310575909085"}},"outputId":"5b741025-994f-412f-9d7e-5faa38cf5cfd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Example 0\n","Input: [0, 1]\n","Continuation: [1, 0, 1, 0, 1, 0, 1, 0, 1]\n","\n","Example 1\n","Input: [0, 0, 0, 0, 0, 0, 0, 0]\n","Continuation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","\n","Example 2\n","Input: [1, 1, 1, 1, 1, 1, 1, 1]\n","Continuation: [1, 1, 1, 1, 1, 1, 1, 1, 1]\n","\n","Example 3\n","Input: [1, 0, 1, 0, 1, 0, 1, 0]\n","Continuation: [1, 0, 1, 0, 1, 0, 1, 0, 1]\n","\n","Example 4\n","Input: [0, 1, 0, 1, 0, 1, 0, 1]\n","Continuation: [1, 0, 1, 0, 1, 0, 1, 0, 1]\n","\n","Example 5\n","Input: [0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0]\n","Continuation: [1, 0, 1, 0, 1, 0, 1, 0, 1]\n","\n","Example 6\n","Input: [1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1]\n","Continuation: [1, 0, 1, 0, 1, 0, 1, 0, 1]\n","\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"PbH4Tr1uzgiL"},"execution_count":null,"outputs":[]}]}