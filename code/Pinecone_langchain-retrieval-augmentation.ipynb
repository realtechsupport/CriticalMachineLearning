{"cells":[{"cell_type":"markdown","metadata":{"id":"f0Qrii3fJYbS"},"source":["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pinecone-io/examples/blob/master/docs/langchain-retrieval-augmentation.ipynb) [![Open nbviewer](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/nbviewer-shield.svg)](https://nbviewer.org/github/pinecone-io/examples/blob/master/docs/langchain-retrieval-augmentation.ipynb)"]},{"cell_type":"markdown","metadata":{"id":"dQRA1HWOJYbU"},"source":["#### [LangChain Handbook](https://pinecone.io/learn/langchain)\n","\n","# Retrieval Augmentation\n","\n","**L**arge **L**anguage **M**odels (LLMs) have a data freshness problem. The most powerful LLMs in the world, like GPT-4, have no idea about recent world events.\n","\n","The world of LLMs is frozen in time. Their world exists as a static snapshot of the world as it was within their training data.\n","\n","A solution to this problem is *retrieval augmentation*. The idea behind this is that we retrieve relevant information from an external knowledge base and give that information to our LLM. In this notebook we will learn how to do that.\n","\n","[![Open full notebook](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/full-link.svg)](https://github.com/pinecone-io/examples/blob/master/learn/generation/langchain/handbook/05-langchain-retrieval-augmentation.ipynb)\n","\n","To begin, we must install the prerequisite libraries that we will be using in this notebook."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0_4wHAWtmAvJ"},"outputs":[],"source":["!pip install -qU \\\n","  langchain==0.1.1 \\\n","  langchain-community==0.0.13 \\\n","  openai==0.27.7 \\\n","  tiktoken==0.4.0 \\\n","  pinecone-client==3.1.0 \\\n","  pinecone-datasets==0.7.0"]},{"cell_type":"markdown","metadata":{"id":"rwnxkKqeRkVh"},"source":["---\n","\n","ðŸš¨ _Note: the above `pip install` is formatted for Jupyter notebooks. If running elsewhere you may need to drop the `!`._\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"RaVQA7J0JYbV"},"source":["## Building the Knowledge Base\n","\n","We will download a pre-embedding dataset from `pinecone-datasets`. Allowing us to skip the embedding and preprocessing steps, if you'd rather work through those steps you can find the [full notebook here](https://colab.research.google.com/github/pinecone-io/examples/blob/master/docs/langchain-retrieval-augmentation.ipynb)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LeAmSrjvKJrV"},"outputs":[],"source":["import pinecone_datasets\n","\n","dataset = pinecone_datasets.load_dataset('wikipedia-simple-text-embedding-ada-002-100K')\n","dataset.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_WXZkwUbKuXK"},"outputs":[],"source":["len(dataset)"]},{"cell_type":"markdown","metadata":{"id":"jDjJHsHUKv28"},"source":["We'll format the dataset ready for upsert and reduce what we use to a subset of the full dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bmUgxFlQK1Ow"},"outputs":[],"source":["# we drop sparse_values as they are not needed for this example\n","dataset.documents.drop(['metadata'], axis=1, inplace=True)\n","dataset.documents.rename(columns={'blob': 'metadata'}, inplace=True)\n","# we will use rows of the dataset up to index 30_000\n","dataset.documents.drop(dataset.documents.index[30_000:], inplace=True)\n","len(dataset)"]},{"cell_type":"markdown","metadata":{"id":"QPUmWYSA43eC"},"source":["Now we move on to initializing our Pinecone vector database."]},{"cell_type":"markdown","metadata":{"id":"AZLtY6dfRkVi"},"source":["## Creating an Index\n","\n","Now the data is ready, we can set up our index to store it.\n","\n","We begin by initializing our connection to Pinecone. To do this we need a [free API key](https://app.pinecone.io)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"40lWz9y1RkVj"},"outputs":[],"source":["import os\n","from pinecone import Pinecone\n","\n","# initialize connection to pinecone (get API key at app.pinecone.io)\n","api_key = os.environ.get('PINECONE_API_KEY') or 'PINECONE_API_KEY'\n","\n","# configure client\n","pc = Pinecone(api_key=api_key)"]},{"cell_type":"markdown","metadata":{"id":"zclHyZVURkVj"},"source":["Now we setup our index specification, this allows us to define the cloud provider and region where we want to deploy our index. You can find a list of all [available providers and regions here](https://docs.pinecone.io/docs/projects)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NhoaHWrSRkVj"},"outputs":[],"source":["from pinecone import ServerlessSpec\n","\n","cloud = os.environ.get('PINECONE_CLOUD') or 'aws'\n","region = os.environ.get('PINECONE_REGION') or 'us-east-1'\n","\n","spec = ServerlessSpec(cloud=cloud, region=region)"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":["parameters"],"id":"XgdOWJJkRkVj"},"outputs":[],"source":["index_name = 'langchain-retrieval-augmentation-fast'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9pT9C4nW4vwo"},"outputs":[],"source":["import time\n","\n","if index_name in pc.list_indexes().names():\n","    pc.delete_index(index_name)\n","\n","# we create a new index\n","pc.create_index(\n","        index_name,\n","        dimension=1536,  # dimensionality of text-embedding-ada-002\n","        metric='dotproduct',\n","        spec=spec\n","    )\n","\n","# wait for index to be initialized\n","while not pc.describe_index(index_name).status['ready']:\n","    time.sleep(1)"]},{"cell_type":"markdown","metadata":{"id":"YgPUwd6REY6z"},"source":["Then we connect to the new index:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RFydARw4EcoQ"},"outputs":[],"source":["index = pc.Index(index_name)\n","# wait a moment for connection\n","time.sleep(1)\n","\n","index.describe_index_stats()"]},{"cell_type":"markdown","metadata":{"id":"0RqIF2mIDwFu"},"source":["We should see that the new Pinecone index has a `total_vector_count` of `0`, as we haven't added any vectors yet.\n","\n","Now we upsert the data to Pinecone:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W-cIOoTWGY1R"},"outputs":[],"source":["for batch in dataset.iter_documents(batch_size=100):\n","    index.upsert(batch)"]},{"cell_type":"markdown","metadata":{"id":"XaF3daSxyCwB"},"source":["We've now indexed everything. We can check the number of vectors in our index like so:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CaEBhsAM22M3"},"outputs":[],"source":["index.describe_index_stats()"]},{"cell_type":"markdown","metadata":{"id":"-8P2PryCy8W3"},"source":["## Creating a Vector Store and Querying\n","\n","Now that we've build our index we can switch over to LangChain. We need to initialize a LangChain vector store using the same index we just built. For this we will also need a LangChain embedding object, which we initialize like so:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FvwwQA4qbcK9"},"outputs":[],"source":["from langchain.embeddings.openai import OpenAIEmbeddings\n","\n","# get openai api key from platform.openai.com\n","OPENAI_API_KEY = os.getenv('OPENAI_API_KEY') or 'OPENAI_API_KEY'\n","\n","model_name = 'text-embedding-ada-002'\n","\n","embed = OpenAIEmbeddings(\n","    model=model_name,\n","    openai_api_key=OPENAI_API_KEY\n",")"]},{"cell_type":"markdown","metadata":{"id":"jKuedXN8bcfA"},"source":["Now initialize the vector store:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qMXlvXOAyJHy"},"outputs":[],"source":["from langchain.vectorstores import Pinecone\n","\n","text_field = \"text\"\n","\n","# switch back to normal index for langchain\n","index = pc.Index(index_name)\n","\n","vectorstore = Pinecone(\n","    index, embed.embed_query, text_field\n",")"]},{"cell_type":"markdown","metadata":{"id":"h1Yg5mKse1bO"},"source":["Now we can query the vector store directly using `vectorstore.similarity_search`:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"COT5s7hcyPiq"},"outputs":[],"source":["query = \"who was Benito Mussolini?\"\n","\n","vectorstore.similarity_search(\n","    query,  # our search query\n","    k=3  # return 3 most relevant docs\n",")"]},{"cell_type":"markdown","metadata":{"id":"ZCvtmREd0pdo"},"source":["All of these are good, relevant results. But what can we do with this? There are many tasks, one of the most interesting (and well supported by LangChain) is called _\"Generative Question-Answering\"_ or GQA.\n","\n","## Generative Question-Answering\n","\n","In GQA we take the query as a question that is to be answered by a LLM, but the LLM must answer the question based on the information it is seeing being returned from the `vectorstore`.\n","\n","To do this we initialize a `RetrievalQA` object like so:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"moCvQR-p0Zsb"},"outputs":[],"source":["from langchain.chat_models import ChatOpenAI\n","from langchain.chains import RetrievalQA\n","\n","# completion llm\n","llm = ChatOpenAI(\n","    openai_api_key=OPENAI_API_KEY,\n","    model_name='gpt-3.5-turbo',\n","    temperature=0.0\n",")\n","\n","qa = RetrievalQA.from_chain_type(\n","    llm=llm,\n","    chain_type=\"stuff\",\n","    retriever=vectorstore.as_retriever()\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KS9sa19K3LkQ"},"outputs":[],"source":["qa.run(query)"]},{"cell_type":"markdown","metadata":{"id":"0qf5e3xf3ggq"},"source":["We can also include the sources of information that the LLM is using to answer our question. We can do this using a slightly different version of `RetrievalQA` called `RetrievalQAWithSourcesChain`:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aYVMGDA13cTz"},"outputs":[],"source":["from langchain.chains import RetrievalQAWithSourcesChain\n","\n","qa_with_sources = RetrievalQAWithSourcesChain.from_chain_type(\n","    llm=llm,\n","    chain_type=\"stuff\",\n","    retriever=vectorstore.as_retriever()\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RXsVEh3S4ZJO"},"outputs":[],"source":["qa_with_sources(query)"]},{"cell_type":"markdown","metadata":{"id":"nMRk_P3Q7l5J"},"source":["Now we answer the question being asked, *and* return the source of this information being used by the LLM.\n","\n","Once done, we can delete the index to save resources."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vNkoIzQjjMTQ"},"outputs":[],"source":["pc.delete_index(index_name)"]},{"cell_type":"markdown","metadata":{"id":"ehJEn68qADoH"},"source":["---"]}],"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/pinecone-io/examples/blob/master/docs/langchain-retrieval-augmentation.ipynb","timestamp":1717421134430}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"}},"nbformat":4,"nbformat_minor":0}